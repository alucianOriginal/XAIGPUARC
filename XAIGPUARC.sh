#!/bin/bash

# ----------------------------------------------------------------------------------
# XAIGPUARC
# Automatischer Build + Run von llama.cpp mit Intel oneAPI / SYCL Backend
# Getestet und Optimiert mit f√ºnf unterschiedlichen ARC Endger√§ten auf Garuda Linux
# Intel ARC A770 (16GiB)/ 750 (8GiB)/
# Single + Dual GPU auf AMD Ryzen 2600/ 2700x/ Intel 6700K @Z170
# Intel 12700h/12650h + A730m 12 GiB + 6GiB /
# Intel Core 155H + ARC iGPU (16GiB RAM/ 11,5 GiB-VRAM)
# ----------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------
#-Globale Variablen f√ºr Build-Verzeichnis (werden in auto_select_device gesetzt)-
DEVICE="Unknown"
PRECISION="FP16"

# -- [0] Umgebung vorbereiten ------------------------------------------------------
prepare_environment() {
    echo "üß© Preparing environment..."

    # -oneAPI Umgebung laden-
    source /opt/intel/oneapi/setvars.sh

    # -Pr√ºfen ob Compiler existiert-
    if ! command -v icx &>/dev/null; then
        which icx || echo "icx nicht gefunden"
        echo "‚ùå Intel compiler (icx/icpx) not found. Check your oneAPI installation."
        exit 1
    fi
    echo "‚úÖ oneAPI environment loaded."
}

# -- [1] Projekt-Setup -------------------------------------------------------------
setup_project() {
    echo "üì¶ Setting up llama.cpp project..."

    # Vorbeugung f√ºr ungebundene variable Fehler
    DEVICE="${DEVICE:-ARC}"

    if [ ! -d "llama.cpp" ]; then
        echo "üì¶ Cloning llama.cpp ..."
        git clone https://github.com/ggerganov/llama.cpp.git || exit 1
    fi

    cd llama.cpp || exit 1

    # -Build-Verzeichnis erstellen (Ger√§t/Pr√§zision-spezifisch)-
    mkdir -p "build_${DEVICE}_${PRECISION}"
    cd "build_${DEVICE}_${PRECISION}"

    echo "‚úÖ llama.cpp ready."
}


# -- [2] Build-Konfiguration -------------------------------------------------------

configure_build() {
    echo "‚öôÔ∏è Configuring build..."

    local USE_FP16=${1:-0}

    #-Cache leeren f√ºr sauberen Rebuild-
    rm -rf CMakeCache.txt CMakeFiles

    if [ "$USE_FP16" -eq 1 ]; then
        echo " Building with FP16 (GGML_SYCL_F16=ON)"
        cmake .. \
          -DGGML_SYCL=ON \
          -DGGML_SYCL_F16=ON \
          -DGGML_SYCL_BACKEND=INTEL \
          -DCMAKE_C_COMPILER=icx \
          -DCMAKE_CXX_COMPILER=icpx \
          -DCMAKE_BUILD_TYPE=Release

    # Wenn FP16 nicht verf√ºgbar nutze FP32
    else
        echo " Building with FP32"
        cmake .. \
          -DGGML_SYCL=ON \
          -DGGML_SYCL_BACKEND=INTEL \
          -DCMAKE_C_COMPILER=icx \
          -DCMAKE_CXX_COMPILER=icpx \
          -DCMAKE_BUILD_TYPE=Release
    fi

    if [ $? -ne 0 ]; then
        echo "‚ùå CMake configuration failed."
        exit 1
    fi
}



# -- [3] Kompilieren ----------------------------------------------------------------
compile_project() {
    echo "üî® Compiling llama.cpp for ARC ${DEVICE} ..."
    cmake --build . \
          --config Release \
          -- -j"$(nproc)" -v || {
        echo "‚ùå Build failed."
        exit 1
    }
    echo "‚úÖ Compilation done."
}

# -- [4] Ger√§t automatisch ausw√§hlen-------------------------------------------------
auto_select_device() {

    echo "üîç Detecting available SYCL / Level Zero devices ...${GPU_ID}"

    # -Liste Ger√§te-
    if [ ! -x "./bin/llama-ls-sycl-device" ]; then
        echo "‚öôÔ∏è Building llama-ls-sycl-device for device detection ..."
        export ONEAPI_DEVICE_SELECTOR="level_zero:0"
        DEVICE="ARC" # Standard-Fallback
        echo "‚ö†Ô∏è llama-ls-sycl-device Binary fehlt. Fallback auf ARC dGPU (Device 0)"
        return
    fi

    #-Liste Ger√§te auf-
    local DEVICES
    DEVICES=$(./bin/llama-ls-sycl-device 2>/dev/null)

    if [ -z "$DEVICES" ]; then
        echo "‚ö†Ô∏è No SYCL devices detected, using CPU fallback."
        export ONEAPI_DEVICE_SELECTOR="opencl:cpu"
        DEVICE="CPU"
        return
    fi

    #-Suche nach ARC dGPU-
    local ARC_ID
    ARC_ID=$(echo "$DEVICES" | grep -i "Intel(R) Arc" | head -n1 | awk '{print $1}')

    #-Suche nach iGPU (Iris/Xe/Graphics/ARC-XE-LPG-iGPU)-
    #-Sie ben√∂tigen Dual Channel RAM Unterst√ºtzung f√ºr die Aktivierung von ARC-XE-LPG+iGPUs!-

    local IGPU_ID
    IGPU_ID=$(echo "$DEVICES" | grep -Ei "Iris|Xe|Graphics" | head -n1 | awk '{print $1}')

    if [ -n "$ARC_ID" ]; then
        export ONEAPI_DEVICE_SELECTOR="level_zero:${ARC_ID}"
        DEVICE="ARC"
        echo "üéØ Using Intel ARC dGPU (Device ${ARC_ID})"
    elif [ -n "$IGPU_ID" ]; then
        export ONEAPI_DEVICE_SELECTOR="level_zero:${IGPU_ID}"
        DEVICE="iGPU"
        echo "üéØ Using Intel Integrated GPU (Device ${IGPU_ID})"
    else
        export ONEAPI_DEVICE_SELECTOR="opencl:cpu"
        DEVICE="CPU"
        echo "‚ö†Ô∏è No suitable GPU found, CPU fallback enabled."
    fi
}

# -- [5] SYCL-Ger√§te pr√ºfen ---------------------------------------------------------
list_sycl_devices() {
    echo "üîç Listing SYCL devices ..."
    if [ -f "./bin/llama-ls-sycl-device" ]; then
        ./bin/llama-ls-sycl-device
    else
        echo "‚ö†Ô∏è llama-ls-sycl-device binary not found. Konnte Ger√§te nicht auflisten."
    fi
}

# -- [6] Modellpfad + Tokenizer vorbereiten -----------------------------------------
prepare_model() {
    MODEL_PATH=${1:-"models/gemma-3-27b-it-abliterated.q4_k_m.gguf"}
    TOKENIZER_PATH="models/tokenizer.model"

    mkdir -p models

    if [ ! -f "$MODEL_PATH" ]; then
        echo "üì• Model nicht gefunden unter **$MODEL_PATH**. Bitte vor Ausf√ºhrung herunterladen!"
    fi

    if [ ! -f "$TOKENIZER_PATH" ]; then
        echo "üì• Tokenizer nicht gefunden unter **$TOKENIZER_PATH**. Bitte vor Ausf√ºhrung herunterladen!"
    fi

    export MODEL_PATH
    export TOKENIZER_PATH
}

# -- [7] Inferenz ausf√ºhren ---------------------------------------------------------
run_inference() {
    local DEFAULT_MODEL_PATH="models/gemma-3-27b-it-abliterated.q4_k_m.gguf"
    local MODEL_PATH_ARG=${1:-$DEFAULT_MODEL_PATH}
    local PROMPT_ARG=${2:-"Hello from SYCL on Intel ARC!"}

    #-Extrahieren der automatisch ausgew√§hlten GPU ID-
    local GPU_ID=$(echo "$ONEAPI_DEVICE_SELECTOR" | awk -F':' '{print $2}')

    echo "üöÄ Running inference on **${DEVICE} (ID: ${GPU_ID})**..."
    ZES_ENABLE_SYSMAN=1 ./bin/llama-cli \
        -no-cnv \
        -m "${MODEL_PATH_ARG}" \
        -p "${PROMPT_ARG}" \
        -n 512 \
        -e \
        -ngl 99 \
        --split-mode none \
        --main-gpu "${GPU_ID}"

    echo "‚úÖ Inference complete."
}

# -- [8] Main Flow ------------------------------------------------------------------
main() {

    # 0. Umgebung vorbereiten
    prepare_environment

    # 1. Projekt-Setup (llama.cpp klonen/wechseln)
    setup_project

    # 2. Build konfigurieren (FP16 oder FP32)
    # Nutzen Sie `main 0` f√ºr FP16 (Standart), `main 1` f√ºr FP32
    configure_build "$@"

    # 3. Kompilieren
    compile_project

    # 4. Ger√§t automatisch ausw√§hlen und ONEAPI_DEVICE_SELECTOR setzen
    auto_select_device # Nutzt das gerade kompilierte Binary

    # 5. SYCL Ger√§te auflisten
    list_sycl_devices

    # 6. Modelldateien vorbereiten (Pfade setzen)
    prepare_model

    # 7. Inferenz ausf√ºhren
    # Optional: Geben Sie einen anderen Modellpfad und Prompt ein:
    # run_inference "models/meine_q4_k_m.gguf" "Was ist der Sinn deines Lebens?"
    run_inference "${MODEL_PATH}" "Welche sind die wichtigsten Vorteile bei der Nutzung von SYCL auf Intel ARC f√ºr KI Inferenzen?"
}

# Skript starten: FP16 (Standart) oder FP32
main ${1:-0}
