# XAIGPUARC
This is a AI Installation Tool specialised for Linux 
(Arch GARUDA Family) and ARC Intel iGPUs and dGPUs / 
also older iGPUS to run over SYCL with Llama.cpp.
There are build in a lot who looks like should not be in, but belive me, some things must be other ways.

Here whe talk about to fast computers, to slow computers and so on.
No matter for you. Matter is funcution! Fast Efizient and from wishes you ever can fell free to ask.
There follow a lot usefull updates who i created from scratch like everytime.

my own way :-)
USE Big Modells!!! Q8/Q6 !!! FP16 half Calculation, double Performance, quad Quality with Q8 and Q6 Modells ONLY!!! 

NOT USEABLE WITH LOW END Modells!!!
On 16GB ARC A770 you will youse the Math Tutor and no other for two years of reasearch. ;-) 
I hope you like the small builds and use solar 10.7b Q6 IQ for the first time, llama3 12b also works in Q6 fine. 
11,5 Gib VRAM needet for that. If you use you normal RAM, try to make him fast as possible. 
Testest well with 718 Gib/s Bandwith on Claw Quad Cannel Meteor Lake Mod.

If you are a Windows User, i personal recommend to use the AIPlayground Programm from Intel itself! 
You see there is no need to make a Windows Version of XAIGPUARC. 
(Update 1 DEZ.25 Now i think this is way faster in the special cases woh it works fine!!!)

A Pro Version with also own made Code from Scratch is comming soon only for Customers.
Let us make AI usefull and less expensive for your Company.

----------------------------------------------
üíª XAIGPUARC: LLM-Build- und Start-Anleitung f√ºr Intel Arc (SYCL-Backend)
----------------------------------------------

Dieses Tool automatisiert den Build- und Startprozess des popul√§ren 
LLM-Frameworks llama.cpp unter Verwendung des Intel oneAPI SYCL-Backends. Es ist speziell f√ºr Intel
Arc dGPUs (und iGPUs) unter Linux-Distributionen (Debian, Red Hat, Arch, SUSE) optimiert.

----------------------------------------------
‚ö†Ô∏è Wichtige Voraussetzungen (Hard-Dependencies)
----------------------------------------------

Bevor Sie mit der Ausf√ºhrung beginnen, m√ºssen folgende Punkte erf√ºllt sein:

Intel oneAPI Toolkit: Die Intel oneAPI Base- und HPC-Toolkits m√ºssen 
vollst√§ndig auf Ihrem System unter dem Standardpfad /opt/intel/oneapi/ installiert sein.

Das Skript PREXAIGPUARC.sh pr√ºft lediglich, ob die Installation vorhanden ist, es installiert sie nicht selbst.

Dateiposition: Die Skripte (PREXAIGPUARC.sh und XAIGPUARC.sh) m√ºssen sich im 
Hauptverzeichnis Ihres Benutzers (dem Home-Ordner, z.B. /home/benutzername/) befinden.

----------------------------------------------
1. üìÇ Projekt-Setup und Vorbereitung
----------------------------------------------

Folgen Sie diesen Schritten, um die Umgebung vorzubereiten und die Skripte lauff√§hig zu machen.

Schritt 1: Dateien in den Home-Ordner laden

Laden Sie die beiden Skripte PREXAIGPUARC.sh und XAIGPUARC.sh herunter 
und speichern Sie diese direkt in Ihrem Home-Ordner (z.B. /home/ihrname/).

Schritt 2: Ausf√ºhrungsberechtigung erteilen

√ñffnen Sie Ihr Terminal und navigieren Sie mit cd ~ in Ihren Home-Ordner (falls Sie nicht bereits dort sind). 
Geben Sie dann die folgenden Befehle ein, um die Skripte ausf√ºhrbar zu machen:

--------------------------
chmod +x PREXAIGPUARC.sh
chmod +x XAIGPUARC.sh
--------------------------


----------------------------------------------
2. üöÄ Installation und Build starten
----------------------------------------------

F√ºhren Sie nun das Vorbereitungs-Skript aus. Dieses installiert alle notwendigen 
Linux-Entwicklerpakete (wie cmake, git, ccache und libcurl-devel), pr√ºft die oneAPI
Installation und startet dann automatisch den Build-Prozess.

---------------------------------------------
3: Build starten
---------------------------------------------

Geben Sie im Terminal den folgenden Befehl ein:

------------------
./PREXAIGPUARC.sh
------------------

Was passiert jetzt?

Abh√§ngigkeiten: Das Skript installiert fehlende Linux-Pakete.

llama.cpp: Das llama.cpp Repository wird in Ihrem Home-Ordner geklont (~/llama.cpp).
Kompilierung: Das Programm wird kompiliert. 
Der gesamte Code wird in das neue Build-Verzeichnis ~/XAIGPUARC geschrieben.
Dauer: Der Build-Prozess kann je nach Hardware einige Minuten in Anspruch nehmen.


---------------------------------------------
4. üß† Modell-Setup und Inferenz
---------------------------------------------

Nach dem erfolgreichen Build m√ºssen Sie das Large Language Model (LLM) bereitstellen.

Schritt 4: LLM-Datei platzieren

Das Skript erstellt w√§hrend des Builds automatisch einen Ordner ~/llama.cpp/models.

Laden Sie ein GGUF-Modell Ihrer Wahl 
(z.B. ein Mistral- oder Llama-Modell) herunter und legen Sie es in diesem Ordner ab.

----------------------------------------------
Wichtig f√ºr eigene Modelloptionen!!!
----------------------------------------------

Standard-Modellpfad: Das Skript ist standardm√§√üig auf models/openhermes-2.5-mistral-7b.Q4_K_M.gguf eingestellt. 
Um ein anderes Modell zu verwenden, m√ºssen Sie dessen
Namen im Skript XAIGPUARC.sh anpassen (siehe Sektion prepare_model). 
Bitte beachten Sie, das sie beide Eintr√§ge √§ndern m√ºssen!!!

----------------------------------------------
5: Inferenz ausf√ºhren
----------------------------------------------

Sie k√∂nnen die Inferenz (den Modell-Start) direkt √ºber das PREXAIGPUARC.sh-Skript starten, 
indem Sie ihm den Modellpfad und einen Prompt als Argumente √ºbergeben:

./PREXAIGPUARC.sh 1 models/Ihr-Modell.gguf 
"Bitte erkl√§re die Funktion einer Intel Arc GPU in einem Satz."

Argument	Beschreibung	Standardwert
1	FP-Modus: Verwenden Sie 1 f√ºr FP16 (empfohlen f√ºr Arc) oder 0 f√ºr FP32.	1
models/Ihr-Modell.gguf	
Der Pfad zu Ihrem GGUF-Modell (relativ zu ~/llama.cpp).	models/openhermes-2.5-mistral-7b.Q4_K_M.gguf

Prompt	Die Start-Eingabeaufforderung f√ºr das Modell.	"Hello from SYCL on Intel ARC!"

ENDE

-----------------------------------------------
üîß Aktuelle Einschr√§nkungen und bekannte Probleme
-----------------------------------------------

Es werden vom Programm grunds√§tzlich nicht alle Modelle vollst√§ndig Unterst√ºtzt. 
Eine Liste f√ºr funktionierende Modelle wird nachgereicht.
Ich empfehle in diesem Fall das schon eingetragene Modell herunterzuladen und zu nutzen, 
falls der VRAM ihrer ARC GPU √ºber 8 Gigabyte liegt.

Ein Support ist nicht Garantiert und entsprechend den teils langwierigen Testverfahren f√ºr eine Implementierung unterlegen.

Es wird empfohlen m√∂glichst genaue Q8 Modelle zu nutzen, 
um eine sichere Ausf√ºhrung mit akzeptablen Antworten zu gew√§hrleisten. 

Der Bau des Programms kann nun bei wiederholung automatisch √ºbergangen werden. 
Wenn der Bau nicht erfolgreich ist, oder es Probleme mit neuen Modellen gibt,
l√∂schen sie die Ordner und versuchen einen neuen Bau des Programms.

Fehlendes Chat-Interface: Die Chat-Funktion ist noch nicht implementiert; 
Sie m√ºssen den Prompt aktuell direkt √ºber die Kommandozeilen-Argumente √ºbergeben.

Manuelle Abh√§ngigkeiten: In einigen F√§llen, insbesondere wenn das PREXAIGPUARC.sh-Skript 
die erforderlichen oneAPI-Bibliotheken nicht findet, kann es notwendig sein,
die relevanten Pakete wie intel-oneapi-basekit oder unter 
Arch/Garuda das onednn Paket manuell √ºber den Paketmanager zu installieren.
