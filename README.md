# XAIGPUARC
This is a AI Installation Tool specialised for Linux 
(Debian, Red Hat, Arch and Suse Family) and ARC Intel GPUs to run over SYCL with Llama.cpp.
If you are a Windows User, i personal recommend to use the AIPlayground Programm from Intel itself! 
You see there is no need to make a Windows Version of XAIGPUARC.

A Pro Version with also own made Code from Scratch is comming soon only for Customers.
Let us make AI usefull and less expensive for your Company.

----------------------------------------------
üíª XAIGPUARC: LLM-Build- und Start-Anleitung f√ºr Intel Arc (SYCL-Backend)
----------------------------------------------

Dieses Tool automatisiert den Build- und Startprozess des popul√§ren 
LLM-Frameworks llama.cpp unter Verwendung des Intel oneAPI SYCL-Backends. Es ist speziell f√ºr Intel
Arc dGPUs (und iGPUs) unter Linux-Distributionen (Debian, Red Hat, Arch, SUSE) optimiert.

----------------------------------------------
‚ö†Ô∏è Wichtige Voraussetzungen (Hard-Dependencies)
----------------------------------------------

Bevor Sie mit der Ausf√ºhrung beginnen, m√ºssen folgende Punkte erf√ºllt sein:

Intel oneAPI Toolkit: Die Intel oneAPI Base- und HPC-Toolkits m√ºssen 
vollst√§ndig auf Ihrem System unter dem Standardpfad /opt/intel/oneapi/ installiert sein.

Das Skript PREXAIGPUARC.sh pr√ºft lediglich, ob die Installation vorhanden ist, es installiert sie nicht selbst.

Dateiposition: Die Skripte (PREXAIGPUARC.sh und XAIGPUARC.sh) m√ºssen sich im 
Hauptverzeichnis Ihres Benutzers (dem Home-Ordner, z.B. /home/benutzername/) befinden.

----------------------------------------------
1. üìÇ Projekt-Setup und Vorbereitung
----------------------------------------------

Folgen Sie diesen Schritten, um die Umgebung vorzubereiten und die Skripte lauff√§hig zu machen.

Schritt 1: Dateien in den Home-Ordner laden

Laden Sie die beiden Skripte PREXAIGPUARC.sh und XAIGPUARC.sh herunter 
und speichern Sie diese direkt in Ihrem Home-Ordner (z.B. /home/ihrname/).

Schritt 2: Ausf√ºhrungsberechtigung erteilen

√ñffnen Sie Ihr Terminal und navigieren Sie mit cd ~ in Ihren Home-Ordner (falls Sie nicht bereits dort sind). 
Geben Sie dann die folgenden Befehle ein, um die Skripte ausf√ºhrbar zu machen:

--------------------------
chmod +x PREXAIGPUARC.sh
chmod +x XAIGPUARC.sh
--------------------------


----------------------------------------------
2. üöÄ Installation und Build starten
----------------------------------------------

F√ºhren Sie nun das Vorbereitungs-Skript aus. Dieses installiert alle notwendigen 
Linux-Entwicklerpakete (wie cmake, git, ccache und libcurl-devel), pr√ºft die oneAPI
Installation und startet dann automatisch den Build-Prozess.

---------------------------------------------
3: Build starten
---------------------------------------------

Geben Sie im Terminal den folgenden Befehl ein:

------------------
./PREXAIGPUARC.sh
------------------

Was passiert jetzt?

Abh√§ngigkeiten: Das Skript installiert fehlende Linux-Pakete.

llama.cpp: Das llama.cpp Repository wird in Ihrem Home-Ordner geklont (~/llama.cpp).
Kompilierung: Das Programm wird kompiliert. 
Der gesamte Code wird in das neue Build-Verzeichnis ~/XAIGPUARC geschrieben.
Dauer: Der Build-Prozess kann je nach Hardware einige Minuten in Anspruch nehmen.


---------------------------------------------
4. üß† Modell-Setup und Inferenz
---------------------------------------------

Nach dem erfolgreichen Build m√ºssen Sie das Large Language Model (LLM) bereitstellen.

Schritt 4: LLM-Datei platzieren

Das Skript erstellt w√§hrend des Builds automatisch einen Ordner ~/llama.cpp/models.

Laden Sie ein GGUF-Modell Ihrer Wahl 
(z.B. ein Mistral- oder Llama-Modell) herunter und legen Sie es in diesem Ordner ab.

----------------------------------------------
Wichtig f√ºr eigene Modelloptionen!!!
----------------------------------------------

Standard-Modellpfad: Das Skript ist standardm√§√üig auf models/openhermes-2.5-mistral-7b.Q4_K_M.gguf eingestellt. 
Um ein anderes Modell zu verwenden, m√ºssen Sie dessen
Namen im Skript XAIGPUARC.sh anpassen (siehe Sektion prepare_model). 
Bitte beachten Sie, das sie beide Eintr√§ge √§ndern m√ºssen!!!

----------------------------------------------
5: Inferenz ausf√ºhren
----------------------------------------------

Sie k√∂nnen die Inferenz (den Modell-Start) direkt √ºber das PREXAIGPUARC.sh-Skript starten, 
indem Sie ihm den Modellpfad und einen Prompt als Argumente √ºbergeben:

./PREXAIGPUARC.sh 1 models/Ihr-Modell.gguf 
"Bitte erkl√§re die Funktion einer Intel Arc GPU in einem Satz."

Argument	Beschreibung	Standardwert
1	FP-Modus: Verwenden Sie 1 f√ºr FP16 (empfohlen f√ºr Arc) oder 0 f√ºr FP32.	1
models/Ihr-Modell.gguf	
Der Pfad zu Ihrem GGUF-Modell (relativ zu ~/llama.cpp).	models/openhermes-2.5-mistral-7b.Q4_K_M.gguf

Prompt	Die Start-Eingabeaufforderung f√ºr das Modell.	"Hello from SYCL on Intel ARC!"

ENDE

-----------------------------------------------
üîß Aktuelle Einschr√§nkungen und bekannte Probleme
-----------------------------------------------

Es werden vom Programm grunds√§tzlich nicht alle Modelle vollst√§ndig Unterst√ºtzt. 
Eine Liste f√ºr funktionierende Modelle wird nachgereicht.
Ich empfehle in diesem Fall das schon eingetragene Modell herunterzuladen und zu nutzen, 
falls der VRAM ihrer ARC GPU √ºber 8 Gigabyte liegt.

Ein Support ist nicht Garantiert und entsprechend den teils langwierigen Testverfahren f√ºr eine Implementierung unterlegen.

Es wird empfohlen m√∂glichst genaue Q8 Modelle zu nutzen, 
um eine sichere Ausf√ºhrung mit akzeptablen Antworten zu gew√§hrleisten. 

Der Bau des Programms kann nun bei wiederholung automatisch √ºbergangen werden. 
Wenn der Bau nicht erfolgreich ist, oder es Probleme mit neuen Modellen gibt,
l√∂schen sie die Ordner und versuchen einen neuen Bau des Programms.

Fehlendes Chat-Interface: Die Chat-Funktion ist noch nicht implementiert; 
Sie m√ºssen den Prompt aktuell direkt √ºber die Kommandozeilen-Argumente √ºbergeben.

Manuelle Abh√§ngigkeiten: In einigen F√§llen, insbesondere wenn das PREXAIGPUARC.sh-Skript 
die erforderlichen oneAPI-Bibliotheken nicht findet, kann es notwendig sein,
die relevanten Pakete wie intel-oneapi-basekit oder unter 
Arch/Garuda das onednn Paket manuell √ºber den Paketmanager zu installieren.
